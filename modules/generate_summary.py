import torch
from transformers import GPTNeoXForCausalLM, AutoTokenizer
from codecarbon import EmissionsTracker
import time
import re


def generate_summary(text: str, optimized: bool = False) -> dict:
    """
    GÃ©nÃ¨re un rÃ©sumÃ© de 10-15 mots Ã  partir d'un texte en anglais.

    Args:
        text: Le texte Ã  rÃ©sumer (max 4000 caractÃ¨res)
        optimized: Si True, utilise la version optimisÃ©e

    Returns:
        dict contenant:
            - summary: le rÃ©sumÃ© gÃ©nÃ©rÃ©
            - word_count: nombre de mots du rÃ©sumÃ©
            - latency: temps d'exÃ©cution en ms
            - energy_consumed: Ã©nergie consommÃ©e en Wh
    """

    # Initialisation du tracker d'Ã©missions
    tracker = EmissionsTracker(
        project_name="text_summarization",
        measure_power_secs=1,
        save_to_file=False,
        log_level="error"
    )

    # DÃ©marrage du chronomÃ¨tre
    start_time = time.time()

    # DÃ©marrage du tracker
    tracker.start()

    try:
        # Chargement du modÃ¨le et du tokenizer
        model_name = "EleutherAI/pythia-70m-deduped"

        if optimized:
            # ============= VERSION OPTIMISÃ‰E =============

            tokenizer = AutoTokenizer.from_pretrained(model_name)

            # Chargement en float16 pour rÃ©duire mÃ©moire et accÃ©lÃ©rer
            model = GPTNeoXForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True
            )

            # GPU si disponible
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model = model.to(device)

            # Mode Ã©valuation
            model.eval()

            # CORRECTION 1: Tronquer le texte plus court pour laisser place au prompt
            # et prendre les phrases les plus importantes (dÃ©but + fin)
            text = text.strip()
            if len(text) > 2000:
                # Prend dÃ©but + fin pour capturer intro et conclusion
                text = text[:1000] + " [...] " + text[-1000:]

            # CORRECTION 2: Prompt plus directif avec contrainte forte
            # On force le modÃ¨le Ã  continuer UNIQUEMENT aprÃ¨s le dernier Summary:
            prompt = f"""Text: Scientists discovered new species in Amazon rainforest with unique adaptations.
Summary: Amazon rainforest yields new species with unique biological adaptations.

Text: {text}
Summary:"""

            # Tokenization
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=600)
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # CORRECTION 3: ParamÃ¨tres de gÃ©nÃ©ration plus stricts
            with torch.no_grad():
                with torch.inference_mode():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=18,  # RÃ‰DUIT pour Ã©viter rÃ©pÃ©titions
                        min_new_tokens=10,  # AUGMENTÃ‰ pour forcer minimum
                        do_sample=False,  # Greedy = dÃ©terministe
                        num_beams=1,  # Pas de beam search
                        repetition_penalty=1.5,  # AJOUTÃ‰: pÃ©nalise les rÃ©pÃ©titions
                        no_repeat_ngram_size=3,  # AJOUTÃ‰: empÃªche rÃ©pÃ©tition de 3+ mots
                        temperature=1.0,
                        pad_token_id=tokenizer.eos_token_id,
                        eos_token_id=tokenizer.eos_token_id,
                        use_cache=True
                    )

        else:
            # ============= VERSION NON-OPTIMISÃ‰E (AMÃ‰LIORÃ‰E) =============

            tokenizer = AutoTokenizer.from_pretrained(model_name)

            # Float32 complet
            model = GPTNeoXForCausalLM.from_pretrained(model_name)

            # CPU forcÃ©
            device = "cpu"
            model = model.to(device)

            # CORRECTION 4: MÃªme troncature pour fairness
            text = text.strip()
            if len(text) > 2000:
                text = text[:1000] + " [...] " + text[-1000:]

            # CORRECTION 5: MÃªme prompt pour comparaison Ã©quitable
            prompt = f"""Text: Scientists discovered new species in Amazon rainforest with unique adaptations.
Summary: Amazon rainforest yields new species with unique biological adaptations.

Text: {text}
Summary:"""

            # Tokenization
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=600)
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # CORRECTION 6: ParamÃ¨tres amÃ©liorÃ©s mais toujours plus lents
            outputs = model.generate(
                **inputs,
                max_new_tokens=18,
                min_new_tokens=10,
                do_sample=True,  # Sampling (plus crÃ©atif mais moins stable)
                num_beams=2,  # Beam search (plus lent)
                repetition_penalty=1.3,  # Moins strict qu'optimisÃ©
                no_repeat_ngram_size=2,  # Moins strict qu'optimisÃ©
                temperature=0.8,  # TempÃ©rature modÃ©rÃ©e
                top_p=0.92,  # Nucleus sampling
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        # ============= POST-TRAITEMENT AMÃ‰LIORÃ‰ =============

        # DÃ©codage
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # CORRECTION 7: Extraction plus robuste du rÃ©sumÃ©
        # On prend SEULEMENT ce qui vient aprÃ¨s le DERNIER "Summary:"
        if "Summary:" in generated_text:
            parts = generated_text.split("Summary:")
            # Prend la derniÃ¨re partie (celle gÃ©nÃ©rÃ©e, pas celle du prompt)
            summary = parts[-1].strip()
        else:
            # Fallback: prend tout aprÃ¨s le prompt
            summary = generated_text[len(prompt):].strip()

        # CORRECTION 8: Nettoyage plus agressif
        # EnlÃ¨ve les retours Ã  la ligne, tabs, espaces multiples
        summary = re.sub(r'\s+', ' ', summary).strip()

        # EnlÃ¨ve les rÃ©pÃ©titions de mots consÃ©cutifs (ex: "the the the")
        summary = re.sub(r'\b(\w+)(\s+\1\b)+', r'\1', summary, flags=re.IGNORECASE)

        # Prend seulement la premiÃ¨re phrase (avant le premier . ! ou ?)
        summary = re.split(r'[.!?]', summary)[0].strip()

        # CORRECTION 9: VÃ©rification de qualitÃ© du rÃ©sumÃ©
        words = summary.split()

        # Si commence par des mots du prompt, les enlever
        prompt_words = ["text", "summary", "summarize"]
        while words and words[0].lower() in prompt_words:
            words.pop(0)

        # EnlÃ¨ve les caractÃ¨res spÃ©ciaux en dÃ©but/fin
        if words:
            words[0] = words[0].lstrip(':-,')
            words[-1] = words[-1].rstrip(':-,')

        # Limite stricte Ã  15 mots
        if len(words) > 15:
            words = words[:15]

        # CORRECTION 10: Fallback intelligent si rÃ©sumÃ© invalide
        if len(words) < 10 or len(words) > 15:
            # Extraire les premiÃ¨res phrases importantes du texte original
            original_sentences = re.split(r'[.!?]', text)
            # Nettoyer les phrases
            original_sentences = [s.strip() for s in original_sentences if len(s.strip()) > 20]

            if original_sentences:
                # Prendre la premiÃ¨re phrase et la tronquer Ã  12-15 mots
                first_sentence = original_sentences[0]
                words = first_sentence.split()[:14]
                # S'assurer qu'on a au moins 10 mots
                if len(words) < 10 and len(original_sentences) > 1:
                    words.extend(original_sentences[1].split())
                words = words[:15]  # Max 15 mots

        # Reconstruction du rÃ©sumÃ© final
        summary = ' '.join(words)

        # Capitalise la premiÃ¨re lettre
        if summary:
            summary = summary[0].upper() + summary[1:]

        # Compte final des mots
        word_count = len(summary.split())

    finally:
        # ArrÃªt du tracker
        emissions_data = tracker.stop()

        # Calcul de la latence en millisecondes
        latency = round((time.time() - start_time) * 1000, 2)

        # Conversion en Wh
        if emissions_data:
            energy_consumed = emissions_data * 1000
        else:
            energy_consumed = 0.0

    # Retour des rÃ©sultats
    return {
        "summary": summary,
        "word_count": word_count,
        "latency": latency,
        "energy_consumed": round(energy_consumed, 6)
    }


# Fonction de test
def test_summary():
    """Fonction de test avec exemples"""

    test_text = """Climate change is one of the most pressing issues facing our planet today. 
    Rising global temperatures are causing ice caps to melt, sea levels to rise, 
    and weather patterns to become more extreme. Scientists warn that without 
    immediate action to reduce greenhouse gas emissions, the consequences could 
    be catastrophic for future generations."""

    test_text2 = """The ocean covering more than seventy percent of Earth's surface has long been 
    both a source of wonder and a foundation for human civilization. Marine ecosystems are 
    increasingly under threat from rising temperatures, plastic pollution, and overfishing. 
    International agreements aim to reduce carbon emissions and protect marine biodiversity."""

    print("=" * 60)
    print("TEST 1: Climate change")
    print("=" * 60)

    print("\nðŸ”´ Version NON-OPTIMISÃ‰E:")
    result_non_opt = generate_summary(test_text, optimized=False)
    print(f"RÃ©sumÃ©: {result_non_opt['summary']}")
    print(f"Mots: {result_non_opt['word_count']}")
    print(f"Latence: {result_non_opt['latency']} ms")
    print(f"Ã‰nergie: {result_non_opt['energy_consumed']} Wh")

    print("\nâœ… Version OPTIMISÃ‰E:")
    result_opt = generate_summary(test_text, optimized=True)
    print(f"RÃ©sumÃ©: {result_opt['summary']}")
    print(f"Mots: {result_opt['word_count']}")
    print(f"Latence: {result_opt['latency']} ms")
    print(f"Ã‰nergie: {result_opt['energy_consumed']} Wh")

    # Calcul des gains
    if result_non_opt['latency'] > 0:
        latency_gain = round(((result_non_opt['latency'] - result_opt['latency']) / result_non_opt['latency']) * 100, 2)
        print(f"\nâš¡ Gain de latence: {latency_gain}%")

    print("\n" + "=" * 60)
    print("TEST 2: Ocean")
    print("=" * 60)

    print("\nðŸ”´ Version NON-OPTIMISÃ‰E:")
    result_non_opt2 = generate_summary(test_text2, optimized=False)
    print(f"RÃ©sumÃ©: {result_non_opt2['summary']}")
    print(f"Mots: {result_non_opt2['word_count']}")

    print("\nâœ… Version OPTIMISÃ‰E:")
    result_opt2 = generate_summary(test_text2, optimized=True)
    print(f"RÃ©sumÃ©: {result_opt2['summary']}")
    print(f"Mots: {result_opt2['word_count']}")

# DÃ©commenter pour tester
# if __name__ == "__main__":
#     test_summary()